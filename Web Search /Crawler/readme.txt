Script - 
The actual python script is "crawler-2.py". 
It can be run from the terminal with the python3 command - "python3 crawler-2.py"

The Output(crawl_log files) are stored in the files given below for each seed list respectively - 
test1.txt  
test2.txt

Packages - 
BeautifulSoup - beautifulsoup4 version - 4.12.3

Note Regarding Output - (Also explained/demonstrated during dmeo) - 
-> The crawler has a configurable time limit which is set per run for each of the 2 seed-lists. 
-> In the latest iteration before the demo, the crawler was ran for 2 hours on seed-list-1 generating a total of 1945 pages visited.
-> It was then very briefly run on seed-list-2 to show it wasn't breaking and the code flow was as intended. 
-> Can configure the crawler to run for 5 or more hours as well for each seed-list and that should be enough to generate 5000 pages. 